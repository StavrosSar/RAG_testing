{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c15b006-6d5d-4abe-8c08-79a73786e56d",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b39074-0d52-4d5d-9977-db9e00752ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import pdfplumber\n",
    "\n",
    "PROJECT_DIR = Path(\".\")  # Ï„ÏÎ­Î¾Îµ notebook Î±Ï€ÏŒ Ï„Î¿ root Ï„Î¿Ï… project\n",
    "PDF_DIR = PROJECT_DIR / \"data_raw\"\n",
    "TXT_DIR = PROJECT_DIR / \"data_text\"\n",
    "OUT_DIR = PROJECT_DIR / \"data_processed\"\n",
    "OUT_JSONL = OUT_DIR / \"chunks.jsonl\"\n",
    "\n",
    "CHUNK_SIZE = 2200\n",
    "OVERLAP = 250\n",
    "MIN_CHUNK_CHARS = 200\n",
    "\n",
    "print(\"PDF_DIR:\", PDF_DIR.resolve(), \"exists:\", PDF_DIR.exists())\n",
    "print(\"PDF count:\", len(list(PDF_DIR.glob(\"*.pdf\"))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8087caf0-6ea6-43e3-b310-27684172bc2f",
   "metadata": {},
   "source": [
    "# PDF â†’ TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271736d3-adba-4222-9cad-77953ca227c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TXT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for pdf_path in PDF_DIR.glob(\"*.pdf\"):\n",
    "    all_text = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                all_text.append(text)\n",
    "\n",
    "    output_txt = TXT_DIR / f\"{pdf_path.stem}.txt\"\n",
    "    output_txt.write_text(\"\\n\\n\".join(all_text), encoding=\"utf-8\")\n",
    "    print(f\"âœ… {pdf_path.name} -> {output_txt.name}\")\n",
    "\n",
    "print(\"ðŸŽ‰ PDF -> TXT done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee35f20-43bb-4f93-b98c-078ba93dc151",
   "metadata": {},
   "source": [
    "# Cleaning + chunking funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1158bc0-6c0b-48ea-b0be-43778026a2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s: str) -> str:\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
    "    s = re.sub(r\"[ \\t]{2,}\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = OVERLAP):\n",
    "    if overlap >= chunk_size:\n",
    "        raise ValueError(\"overlap must be smaller than chunk_size\")\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    n = len(text)\n",
    "\n",
    "    while start < n:\n",
    "        end = min(start + chunk_size, n)\n",
    "        chunk = text[start:end].strip()\n",
    "        if len(chunk) >= MIN_CHUNK_CHARS:\n",
    "            chunks.append((start, end, chunk))\n",
    "        start = end - overlap\n",
    "        if start < 0:\n",
    "            start = 0\n",
    "        if end == n:\n",
    "            break\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a575a36-502a-427a-8577-70d1742e768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TXT â†’ chunks.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f7a5db-248f-46e4-86ef-72f0cb7b3245",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "txt_files = sorted(TXT_DIR.glob(\"*.txt\"))\n",
    "print(\"TXT files:\", len(txt_files))\n",
    "\n",
    "rows = []\n",
    "for txt_path in txt_files:\n",
    "    doc_id = txt_path.stem\n",
    "    raw = txt_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    text = clean_text(raw)\n",
    "\n",
    "    for j, (start_char, end_char, chunk) in enumerate(chunk_text(text)):\n",
    "        rows.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"source\": str(txt_path),\n",
    "            \"chunk_id\": f\"{doc_id}_{j:05d}\",\n",
    "            \"text\": chunk,\n",
    "            \"start_char\": start_char,\n",
    "            \"end_char\": end_char\n",
    "        })\n",
    "\n",
    "with open(OUT_JSONL, \"w\", encoding=\"utf-8\") as f:\n",
    "    for r in rows:\n",
    "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"âœ… Wrote:\", OUT_JSONL)\n",
    "print(\"Total chunks:\", len(rows))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84048c0e-6910-48d5-af44-73d3298c18d5",
   "metadata": {},
   "source": [
    "# Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1f9aac-8827-4ca8-b01f-e7f320b337ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in rows[:3]:\n",
    "    print(\"\\n---\", r[\"chunk_id\"], \"---\")\n",
    "    print(r[\"text\"][:600])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
